# -*- coding: utf-8 -*-
"""NLP03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ktz-n9GSdB7C2Pv351hjg-a3RHUgznuE
"""

# 지금까지 기계가 자연어처리를 원활하게 할 수 있도록 벡터화를 계속 했음

"""## BOW (Bag of words)
중복을 제거하지 않고 단어들의 빈도의 정보를 보여줌.  
어순에 따라 달라지는 의미를 반영하지 못함
e.g) i ate apple. = apple ate i.  

1. 통계와 머신러닝을 활용한 방법
2. 인공 신경망을 활용한 방법

doc1 = 'John likes to watch movies. Mary likes movies too.'

BoW1 = {"John":1, "likes":2, "to":1, "watch":1, "movies":2, "Mary":1, "too":1}

doc2 = 'Mary also likes to watch football games.'  
BoW2 = {"Mary":1, "also":1, "likes":1, "to":1, "watch":1, "football":1, "games":1}

doc3 = 'John likes to watch movies. Mary likes movies too. Mary also likes to watch football games.'  
BoW3 = {"John":1, "likes":3, "to":2, "watch":2, "movies":2, "Mary":2, "too":1, "also":1, "football":1, "games":1};

## Keras Tokenizer를 활용한 BOW
"""

from tensorflow.keras.preprocessing.text import Tokenizer

sentence = ['John likes to watch movies. Mary likes movies too. Mary also likes to watch football games.']

def print_bow(sentence):
  tokenizer = Tokenizer()

  # Create vocabulary
  tokenizer.fit_on_texts(sentence) 

  # Store word with the frequency to 'bow'
  bow = dict(tokenizer.word_counts) 

  # prtin bow
  print("Bag of words :", bow) 

  # Count words(remove the duplicated)
  print('단어장(vocabulary)의 크기 :', len(tokenizer.word_counts)) 

print_bow(sentence)

"""## Scikit-learn CountVectorizer를 활용한 BOW"""

from sklearn.feature_extraction.text import CountVectorizer

sentence = ["John likes to watch movies. Mary likes movies too! Mary also likes to watch football games."]

vector = CountVectorizer()

# 코퍼스(sentence?)로부터 각 단어의 빈도수를 기록
print('Bag of Words : ', vector.fit_transform(sentence).toarray()) 

# 각 단어의 인덱스가 어떻게 부여되는지를 보여줌. -1?
print('각 단어의 인덱스: ', vector.vocabulary_)

print('단어장(vocabulary)의 크기 :', len(vector.vocabulary_))

"""## DTM (Document-Term Matrix)
각 문서에 등장한 각 단어의 빈도수를 행으로  
잘 안쓰는 듯

문서 1 : I like dog  
문서 2 : I like cat  
문서 3 : I like cat I like cat

-    cat    dog    I    like  
문서1   0     1      1     1  
문서2   1     0      1     1  
문서3   2     0      2     2
"""

import pandas as pd

content = [[0,1,1,1],[1,0,1,1],[2,0,2,2]]

df = pd.DataFrame(content)

df.index = ['문서1', '문서2', '문서3']
df.columns = ['cat', 'dog', 'I', 'like']
df

import numpy as np
from numpy import dot
from numpy.linalg import norm

doc1 = np.array([0, 1, 1, 1])
doc2 = np.array([1, 0, 1, 1])
doc3 = np.array([2, 0, 2, 2])

# cos유사도
def cos_sim(A, B):
  return dot(A, B)/(norm(A)*norm(B))

print(cos_sim(doc1, doc2))
print(cos_sim(doc1, doc3))
print(cos_sim(doc2, doc3))

"""DTM에서 코사인 유사도는 0이상 1이하의 값을 가지고, 값이 1에 가까울수록 유사도가 높다고 판단"""



"""## Scikit-learn CountVectorizer활용한 DTM구현"""

from sklearn.feature_extraction.text import CountVectorizer

corpus = [
          'John likes to watch movies',
          'Mary likes movies too',
          'Mary also likes to watch football games',
]

vector = CountVectorizer()

# 코퍼스로부터 각 단어의 빈도수를 기록
print(vector.fit_transform(corpus).toarray()) 

# 각 단어의 인덱스가 어떻게 부여되었는지 보여준다
print(vector.vocabulary_)

"""DTM의 한계
1. 문서 단어수가 늘어날수록 행과 열에 0이 많아진다 -> 지나친 차원의 크기(저장 공간 낭비)
2. 단어 빈도에만 집중(문장끼리 비교할 때) -> 중요성을 알 수 없다. (불용어도)   
=> 중요한거에만 가중치를 줄 순 없을까? => tf-idf

## TF-IDF

모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단하는 것

단어의 빈도  
문서의 빈도의 역수

DTM보다 항상 뛰어나진 않음  
DTM -> TF-IDF 적용하는 식으로 사용
"""

from math import log
import pandas as pd

docs = [
        'John likes to watch movies and Mary likes movies too',
        'James likes to watch TV',
        'Mary also likes to watch football games',
]

vocab = list(set(w for doc in docs for w in doc.split()))
vocab.sort()
print('단어장의 크기 :', len(vocab))
print(vocab)

N = len(docs)
N

def tf(t,d):
  return d.count(t)

def idf(t):
  df = 0
  for doc in docs:
    df += t in doc
  return log(N/(df + 1))+1

def tfidf(t, d):
  return tf(t,d)* idf(t)

"""TF함수를 사용하여 DTM을 만들어보자"""

result = []
for i in range(N): # 각 문서에 대해서 아래 명령을 수행
  result.append([])
  d = docs[i]
  for j in range(len(vocab)): # 13
    t = vocab[j]

    result[-1].append(tf(t,d))

tf_ = pd.DataFrame(result, columns=vocab)
tf_

result = []
for j in range(len(vocab)):
  t = vocab[j]
  result.append(idf(t))

idf_ = pd.DataFrame(result, index = vocab, columns = ["IDF"])
idf_

"""TF-IDF행렬을 출력 DTM에 있는 각 단어의 TF에 각 단어의 iDF를 곱해준 값"""

result = []
for i in range(N):
  result.append([])
  d = docs[i]
  for j in range(len(vocab)):
    t = vocab[j]

    result[-1].append(tfidf(t,d))

tfidf_ = pd.DataFrame(result, columns= vocab)
tfidf_

'John likes to watch movies and Mary likes movies too',
'James likes to watch TV',
'Mary also likes to watch football games',

"""## Scikit-learn TFidVectorizer활용"""

from sklearn.feature_extraction.text import TfidfVectorizer
corpus = [
          'John likes to watch movies and Mary likes movies too',
          'James likes to watch TV',
          'Mary also likes to watch football games',
]

tfidfv = TfidfVectorizer().fit(corpus)
vocab = list(set(tfidfv.vocabulary_.keys()))
vocab.sort()

tfidf_ = pd.DataFrame(tfidfv.transform(corpus).toarray(), columns=vocab)
tfidf_

"""한계 : 단어들의 의미를 알고 싶을 때  
-> LSA / LDA
근데 딥러닝 나와서 잘 안씀. 가벼운 모델에서는 쓴다

LSA  (잠재의미분석)  
전체 코퍼스에서 문자 속 단어들 상의 관계를 찾아내는 자연어 처리 정보검색 기술  
단어와 단어사이, 문서와 문서사이, 단어와 문서사이의 의미적 유사성 점수를 찾아낸다.
"""

import pandas as pd
import numpy as np
import urllib.request
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.corpus import stopwords # 불용어 처리
from nltk.stem import WordNetLemmatizer

!pip install nltk # nltk 설치

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

urllib.request.urlretrieve("https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv",
                           filename="/content/abcnews-data-text.csv")

data = pd.read_csv('/content/abcnews-data-text.csv', error_bad_lines=False)

data.head()

text = data[['headline_text']]
text.head()

text.nunique() # 중복확인

# 중복제거
text.drop_duplicates(inplace=True)
text = text.reset_index(drop=True)
print(len(text))

"""데이터 정제 및 정규화"""

# NLTK 토크나이저를 이용해서 토큰화시키기
text['headline_text'] = text.apply(lambda row:nltk.word_tokenize(row['headline_text']), axis=1)

# 불용어 제거
stop_words = stopwords.words('english')
text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop_words)])

text.head()

# 단어 정규화 과정 길이가 1~2인 단어는 제거하는 전처리
# 단어 정규화 3인칭 단수 표현 -> 1인칭 변환, 과거형 동사 -> 현재형 동사등을 수행
text['headline_text'] = text['headline_text'].apply(lambda x : [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])

# 길이가 1~2인 단어를 제거
text = text['headline_text'].apply(lambda x : [word for word in x if len(word) > 2])
print(text[:5])

# 역토큰화 (토큰화 작업을 역으로 수행)
detokenized_doc=[]
for i in range(len(text)):
  t = ' '.join(text[i])
  detokenized_doc.append(t)

train_data = detokenized_doc

train_data[:5]

# 상위 5000개의 단어만 사용
c_vectorizer = CountVectorizer(stop_words='english', max_features= 5000)
document_term_matrix = c_vectorizer.fit_transform(train_data)

# DTM의 크기
print('행렬의 크기 : ', document_term_matrix.shape) # 문서의 수 X 단어 집합의 크기

"""한계 : 새로운 정보를 업데이트하기 힘들다   
-> 딥러닝(신경망)

## Scikit-learn Truncated SVD 활용
"""

from sklearn.decomposition import TruncatedSVD

n_topics = 10
lsa_model = TruncatedSVD(n_components = n_topics)
lsa_model.fit_transform(document_term_matrix)

print(np.shape(lsa_model.components_))

terms = c_vectorizer.get_feature_names()

def get_topics(components, feature_names, n=5):
  for idx, topic in enumerate(components):
    print("Topic %d:" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])

get_topics(lsa_model.components_, terms)
# LSA에 대한 결과물

"""TF-IDF 행렬 생성"""

# 상위 5000개의 단어만 사용
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)
tf_idf_matrix = tfidf_vectorizer.fit_transform(train_data)

# TF-IDF행렬의 크기를 확인
print('행렬의 크기 : ', tf_idf_matrix.shape)

"""Scikit-learn LDA model활용"""

from sklearn.decomposition import LatentDirichletAllocation
lda_model = LatentDirichletAllocation(n_components = 10, learning_method = 'online', random_state = 777, max_iter=1)
lda_model.fit_transform(tf_idf_matrix)

print(np.shape(lda_model.components_))

# LDA의 결과 토픽과 각 단어의 비중을 출력하자
def get_topics(components, feature_names, n=5):
  for idx, topic in enumerate(components):
    print("Topic %d:" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])

get_topics(lda_model.components_, terms)

!pip install nltk
!pip install gensim

import nltk
nltk.download('abc')
nltk.download('punkt')

"""## English Word2Vec Practice"""

from nltk.corpus import abc
corpus = abc.sents()

print(corpus[:3])

print('코퍼스의 크기 :', len(corpus))

# word2vec training
from gensim.models import Word2Vec

model = Word2Vec(sentences=corpus, size=100, window=5, min_count=5, workers=4, sg=0)
# min_count < n : n보다 빈도수가 적으면 학습하지 않는다
# sg =cbow, 1=skipgram

model_result = model.wv.most_similar('man')

print(model_result)

from gensim.models import KeyedVectors

model.wv.save_word2vec_format('./w2v')
loaded_model = KeyedVectors.load_word2vec_format('./w2v')
print('모델 load완료')

model_result = loaded_model.wv.most_similar('man')

print(model_result) # 같은 결과

'OOV의 문제를 그대로 보유'

loaded_model.most_similar('overacting') 
# overacting: 훈련데이터(단어장)에 없는 단어 -> 임베딩 벡터값을 뽑아낼 수 없다
# KeyError: "word 'overacting' not in vocabulary"

loaded_model.most_similar('memory')

loaded_model.most_similar('memorry')

"""## Korean Word2Vec?"""

!pip install konlpy

import pandas as pd
import matplotlib.pyplot as plt
import urllib.request
from gensim.models.word2vec import Word2Vec
from konlpy.tag import Okt

urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt", filename="ratings.txt")

# naver en rewiew 
# nsmc : 감성분석할 때 주로 쓰는 네이버 영화 리뷰

train_data = pd.read_table('ratings.txt')

train_data[:5] # positive: 1, negative: 0

print(len(train_data)) # 리뷰개수

print(train_data.isnull().values.any())

train_data = train_data.dropna(how='any')
print(train_data.isnull().values.any())

print(len(train_data))

# 한글 외 문자 정규표현식으로 제거하기
train_data['document'] = train_data['document'].str.replace("[^ㄱ-ㅎ ㅏ-ㅣ 가-힣]","")

train_data[:5]

# 불용어 정의
stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']

okt = Okt()
tokenized_data = []
for sentence in train_data['document']:
  temp_x = okt.morphs(sentence, stem=True) # 토큰화
  temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거
  tokenized_data.append(temp_x)

# 리뷰 길이 분포 확인
print('리뷰의 최대 길이 :', max(len(l) for l in tokenized_data))
print('리뷰의 평균 길이 :', sum(map(len, tokenized_data))/len(tokenized_data))
plt.hist([len(s) for s in tokenized_data], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

from gensim.models import Word2Vec
model = Word2Vec(sentences = tokenized_data, size=100, window=5, min_count=5, workers=4, sg=0)

model.wv.vectors.shape

print(model.wv.most_similar("최민식"))

print(model.wv.most_similar("히어로"))

# 모델구조
# 학습(compile)

"""## 사전 훈련된 워드 임베딩"""

import gensim
model = gensim.models.Word2Vec.load('/content/drive/MyDrive/ko.bin')

!pwd

ls

result = model.wv.most_similar("강아지")
print(result)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']
y_train = [1, 0, 0, 1, 1, 0, 1] # 긍정 1 부정 0

# 토 큰 화 
t = Tokenizer()
t.fit_on_texts(sentences)
vocab_size = len(t.word_index) + 1

print(vocab_size)

# 인코딩 
x_encoded = t.texts_to_sequences(sentences)
print(x_encoded)

max_len = max(len(l) for l in x_encoded)
print(max_len)

# Padding
x_train = pad_sequences(x_encoded, maxlen=max_len, padding= 'post')
y_train = np.array(y_train)
print(x_train)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Flatten

model = Sequential()
model.add(Embedding(vocab_size, 4, input_length=max_len))
model.add(Flatten())
model.add(Dense(1, activation="sigmoid"))

model.summary()

model.compile(optimizer="adam", loss='binary_crossentropy', metrics=['acc'])
model.fit(x_train, y_train, epochs=100, verbose=2)

