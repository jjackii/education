# -*- coding: utf-8 -*-
"""NLP05.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BTnHVs_Lhjk4HjrzXVhWs-0dFXV3a6N7

# OCR(Optical character recognition, 광학 문자 인식)
이미지 속 글자 인식 기술 (기계가 읽을 수 있는 문자로 변환)

## 이미지 텍스트 추출
- pytesseract
"""

# pytessetact (python) / pillow :이미지 로드
!pip install pillow pytesseract

!pip install -upgrade pip

from pytesseract import pytesseract
from PIL import Image

def load_image(path, mode=''):
  return Image.open(path)

def run_pytesseract(image, path_engine, config='-I eng'):
  # 자동으로 이 경로로 실행
  pytesseract.tesseract_cmd = path_engine
  # img2string
  result = pytesseract.image_to_string(image, config=config)
  return result

import pytesseract
import cv2
import os
from PIL import Image
from google.colab.patches import cv2_imshow

# 현재위치
!pwd

!sudo apt install tesseract-ocr
!pip install pytesseract

ls

cd ./drive

cd MyDrive/NLP

image = cv2.imread('./test.jpg')

# print(image)
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

filename = '{}.png'.format(os.getpid())
cv2.imwrite(filename, gray)

# pytesseract의 image to string을 써주고, 숫자는 lang ="none"
text = pytesseract.image_to_string(Image.open(filename), lang=None)
os.remove(filename)

print(text)

cv2_imshow(image)

"""## PDF 텍스트 추출
- pdfminer
- pdfplumber (더 간단)
"""

!pip install pdfminer

from io import StringIO
import pdfminer
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfpage import PDFPage

def convert_pdf2txt(path):
  manager = PDFResourceManager()
  file_object = StringIO()
  converter = TextConverter(manager, file_object, laparams=LAParams())
  with open(path, 'rb') as f:
    interpreter = PDFPageInterpreter(manager, converter)
    for page in PDFPage.get_pages(f, maxpages=0, caching=True, check_extractable=True):
      interpreter.process_page(page)
  converter.close()
  text =file_object.getvalue()
  file_object.close()
  return text

convert_pdf2txt('./sample_papers.pdf')

!pip install pdfplumber

import pdfplumber

text = ''
with pdfplumber.open('./sample_papers.pdf') as pdf:
  for page in pdf.pages:
    text += page.extract_text()
text

"""# Recurrent Neural Network (순환 신경망)

https://wikidocs.net/22886

## Keras로 RNN 구현
"""

from keras.models import Sequential
from keras.layers import SimpleRNN

"""RNN층에 대한 코드 (완성된 인공 신경망 X)\
return : 은닉 상태 (출력층 X)\
"""

# sequential 객체 선언 (넌 시퀀스야!!)
model = Sequential()

# 메모리 셀의 최종 시점의 은닉 상태만을 리턴 -> (batch_size, output_dim) 크기의 2D 텐서를 리턴
model.add(SimpleRNN(3, input_shape=(2,10))) # hidden_size, input_shape(timesteps, input_dim)

model.summary() # OutputShape(None)=현 단계에서 batch size 알 수 없다

# batch_size 지정
model = Sequential()

# 메모리 셀의 각 시점(time step)의 은닉 상태값들을 모아서 전체 시퀀스를 리턴
# -> (batch_size, timesteps, output_dim) 크기의 3D 텐서를 리턴
# 하나의 은닉 상태값만을 출력
model.add(SimpleRNN(3, batch_input_shape=(8,2,10)))

model.summary()

model = Sequential()

# 메모리 셀이 모든 시점(time step)에 대해서 은닉 상태값을 출력
model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))
model.summary()

"""## Python으로 RNN 구현 (numpy)

$$ h_t = tanh(W_x X_t + W_h H_{t-1} + b)$$

```
의사코드
hidden_state_t = 0
for input_t in input_length: #각 시점마다 입력을 받는다.
    output_t = tanh(input_t, hidden_stat_t) #각 시점에 대해서 입력과 은닉 상태를 가지고 연산
    hidden_stat_t = output_t # 계산결과는 현재 시점의 은닉 상태가 된다.
```
"""

import numpy as np
timesteps = 10 # 시점의 수
input_dim = 4 # 차원
hidden_size = 8 # 은닉 상태의 크기 (메모리 셀의 용량)

inputs = np.random.random((timesteps, input_dim)) # 2d

hidden_state_t = np.zeros((hidden_size, )) # 초기 은닉 상태는 0(벡터)으로 초기화
# 은닉 상태의 크기 hidden size로 은닉 상태를 만듬

# 초기화 확인
print(hidden_state_t)

# 가중치, 편향 정의
Wx = np.random.random((hidden_size, input_dim)) # (8,4) 입력에 대한 가중치
Wh = np.random.random((hidden_size, hidden_size)) # (8,8) 은닉상태에 대한 가중치 ppt
b = np.random.random((hidden_size,)) # (8,) 편향

print(np.shape(Wx)) # 은닉상태의 크기 * 입력의 차원
print(np.shape(Wh)) # 은낙상태의 크기 * 은닉상태의 크기
print(np.shape(b)) # 은닉상태

# 모든 시점의 은닉 상태를 출력한다고 가정하고, RNN 층을 동작

total_hidden_state = []

# 메모리 셀 동작
for input_t in inputs:
  output_t = np.tanh(np.dot(Wx, input_t)+ np.dot(Wh, hidden_state_t)+b) 
  total_hidden_state.append(list(output_t))
  print(np.shape(total_hidden_state)) #각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)

total_hidden_state = np.stack(total_hidden_state, axis=0)  # (1, 2, 3) 축(차원)이 있을 때 0번째

print(total_hidden_state) # (timesteps , outputdim) 크기

"""## 양방향 순환 신경망 (Bidirectional Recurrent Neural Network)
시점 t에서의 출력값을 예측할 때 이전 시점의 데이터뿐만 아니라, 이후 데이터로도 예측할 수 있다는 아이디어에 기반
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Bidirectional

model = Sequential()
model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences= True), input_shape=(timesteps, input_dim)))

model.summary()

# 은닉층(add) 더 쌓기

model = Sequential()
model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences=True), input_shape=(timesteps, input_dim)))
model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences=True)))
model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences=True)))
model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences=True)))

model.summary()

"""``` 
Quiz
1. Embedding을 사용하여 단어 집합(Vocabulary)의 크기가 5,000이고, 임베딩 벡터의 차원은 100입니다.
2. 은닉층에서 Simple RNN을 사용하여 은닉상태의 크기는 128이다.
3. 훈련에 사용하는 모든 샘플의 길이는 30으로 가정
4. 이진 분류를 수행하는 모델로, 출력층의 뉴련은 1개로 시그모이드 함수를 사용
5. 은닉층은 1개
총 파라미터 갯수를 구하시오.
```

```
Embedding = 5,000(input) * 100(embedding) = 500,000 # input 5000* 100(embadding/차원)
Wx = 100(embedding) * 128(hidden) = 12,800
Wh = 128 * 128 = 16,384
bias(hidden) = 128
Wy = 128
bias(output (y) ) = 1

total = 529,441
```

# 임의의 입력으로 SimpleRNN과 LSTM 이해하기
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import SimpleRNN, LSTM, Bidirectional

# 2d tensor
train_x = [[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]
print(np.shape(train_x)) # 단어 벡터 차원은 5, 문장의 길이가 4인 경우를 가정

# 3d tensor, batch크기 할당
train_x = [[[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]]

# list to array
train_x = np.array(train_x, dtype=np.float32)

print(train_x.shape) # (batch_size, timesteps, input_dim) / # sample1밖에 없으므로 batch size = 1

# Simple, return_sequences=False : 마지막 상태의 hidden layer만 반환
rnn = SimpleRNN(3) # rnn = SimpleRNN(3, return_sequences=False, return_state=False)와 동일

hidden_state = rnn(train_x)

print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))

# return_sequences=True, 모든 시점에 대한 hidden state 반환
rnn = SimpleRNN(3, return_sequences=True)
hidden_state = rnn(train_x)
print('hidden states : {}, shape : {}'.format(hidden_state, hidden_state.shape))

# return_sequences=True, return_state=True 
# 첫번째 출력, 모든 시점에 대한 hidden state 
# 두번째 출력, 마지막 시점의 hidden state (모든 시점의 마지막 상태와 값이 같다)
rnn = SimpleRNN(3, return_sequences=True, return_state=True)
hidden_state, last_state = rnn(train_x)

print('hidden states: {}, shape:{}'.format(hidden_state, hidden_state.shape))
print('last hidden state: {}, shape : {}'.format(last_state, last_state.shape))

# return_sequences=False, return_state=True
# 두 개 출력 모두 마지막 상태의 은닉층을 출력
rnn = SimpleRNN(3, return_sequences=False, return_state=True)
hidden_state, last_state = rnn(train_x)

print('hidden state ; {}, shape: {}'.format(hidden_state, hidden_state.shape))
print('last hidden state : {}, shape : {}'.format(last_state, last_state.shape))

"""# LSTM 이해하기
- 기울기 소실 문제로 인해 simple RNN은 잘 사용하지 않고 LSTM 사용
"""

# return_sequences=False, 마지막 히든 상태
# return_state=True, 마지막 시점의 은닉 상태 + 셀 상태 반환
lstm = LSTM(3, return_sequences=False, return_state=True)
hidden_state, last_state, last_cell_state = lstm(train_x)

print('hidden state : {}, shape :{}'.format(hidden_state, hidden_state.shape))
print('last hidden state: {}, shape : {}'.format(last_state, last_state.shape))
print('last cell state : {}, shape : {}'.format(last_cell_state, last_cell_state.shape))

# return_sequences=True, 모든 시점의 은닉 상태 출력
# return_state=True, 두번째 출력 값, 마지막 상태
lstm = LSTM(3, return_sequences=True, return_state=True)
hidden_state, last_hidden_state, last_cell_state = lstm(train_x)

print('hidden state : {}, shape : {}'.format(hidden_state, hidden_state.shape))
print('last hidden state: {}, shape: {}'.format(last_hidden_state, last_hidden_state.shape))
print('last cell state : {}, shape : {}'.format(last_cell_state, last_cell_state.shape))

"""# Bidirectional(LSTM) 이해하기"""

# 출력되는 은닉 상태 값 고정
k_init = tf.keras.initializers.Constant(value= 0.1)
b_init = tf.keras.initializers.Constant(value=0)
r_init = tf.keras.initializers.Constant(value= 0.1)

# return_sequences=False
# return_state=True
bilstm = Bidirectional(LSTM(3, return_sequences=False, return_state=True, kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))
hidden_state, forward_h, forward_c, backward_h, backward_c = bilstm(train_x)

print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))
print('forward state : {}, shape : {}'.format(forward_h, forward_h.shape))
print('backward state : {}, shape : {}'.format(backward_h, backward_h.shape))

# return_sequences=True, return_state=True
bilstm = Bidirectional(LSTM(3, return_sequences=True, return_state=True, kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))
hidden_state, forward_h, forward_c, backward_h, backward_c = bilstm(train_x)

print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))
print('forward state : {}, shape : {}'.format(forward_h, forward_h.shape))
print('backward state : {}, shape : {}'.format(backward_h, backward_h.shape))

# forward state : [[0.63031393 0.63031393 0.63031393]], shape : (1, 3)
# backward state : [[0.7038734 0.7038734 0.7038734]], shape : (1, 3)

"""- hidden state 를 보는 의미\
model을 설계하다보면 첫번째 은닉층을 가져다 쓸경우도 있고, 두번째 state를 가져다 쓸수도 있고 마지막 층을 가져다 쓸경우도 있는데, 안에 state결과값을 보면 좀더 쉽게 이해가 되서 보고 있습니다!\
정확히는 이제 뒤에 배울 attention개념이 들어가면 이 hidden state를 가지고 점수를 매기기 때문에 return sequence를 True/False에 따라 hidden state가 어떻게 출력되는지 보시면 좋을 것 같습니다\
그럼 hidden state 값을 보면서 알맞은 은닉층을 선택해서 사용할 수 있다는 말씀으로 이해하면 되겠네요

# RNN을 이용한 텍스트 생성 (Text Generation)
- 다 대 일 (many to one) 구조의 RNN을 사용하여 문맥을 반영해 텍스트를 생성하는 모델을 만들어 보자
- '경마장에 있는 말이 뛰고 있다'
- '그의 말이 법이다'
- '가는 말이 고와야 오는 말이 곱다'
"""

import pandas as pd
# 위 3 문장을 재구성하면, 아래와 같이 총 11개의 샘플이 구성됨
content = [['경마장에', '있는'],['경마장에 있는','말이'],['경마장에 있는 말이', '뛰고'],['경마장에 있는 말이 뛰고','있다'],['그의','말이'],['그의 말이','법이다'],['가는', '말이'],['가는 말이','고와야'],['가는 말이 고와야','오는'],['가는 말이 고와야 오는','말이'],['가는 말이 고와야 오는 말이','곱다']]
df = pd.DataFrame(content)
df.index = ['1', '2', '3', '4', '5', '6', '7', '8', '9','10','11']
df.columns = ['X', 'y']
df

"""## 데이터에 대한 이해와 전처리"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
from tensorflow.keras.utils import to_categorical # one-hot encoding

text = """ 경마장에 있는 말이 뛰고 있다 \n
그의 말이 법이다\n
가는 말이 고와야 오는 말이 곱다\n"""

# 단어 집합 생성, 크기 확인
t = Tokenizer()
t.fit_on_texts([text])
vocab_size = len(t.word_index) +1

# 케라스 토크나이저 정수 인코딩은 인덱스가 1부터 시작,
# 케라스 원-핫 인코딩에서 배열의 인덱스가 0부터 시작하기 때문에
# 배열의 크기를 실제 단어 집합의 크기보다 +1로 생성해야함 그래서 미리 +1 선언
print('단어 집합의 크기 : %d'% vocab_size)

# 단어 집합의 정수 인덱스
print(t.word_index)

# 훈련데이터 만들기
sequences = list()
for line in text.split('\n'):
  encoded = t.texts_to_sequences([line])[0]
  for i in range(1, len(encoded)):
    sequence = encoded[:i+1]
    sequences.append(sequence)

print('학습에 사용할 샘플의 갯수 : %d' % len(sequences))

print(sequences)

max_len = max(len(l) for l in sequences) # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력
print('샘플의 최대 길이 : {}'.format(max_len))

# max_len을 기준으로 모자란 길이를 0으로 채워 줌(padding), keras
sequences = pad_sequences(sequences, maxlen = max_len, padding='pre')

print(sequences)

# 각 샘플의 마지막 단어를 label로 분리, numpy
sequences = np.array(sequences)
x = sequences[:, :-1]
y = sequences[:, -1]
# 리스트의 마지막 값을 제외하고 저장한 것은 x
# 리스트의 마지막 값만 저장한 것은 y --> label에 해당

print(x)

print(y) # 모든 샘플에 대한 레이블 출력

# label에 대한 one-hot encoding
y = to_categorical(y, num_classes=vocab_size)

print(y) # 미리 +1을 해서 맨 앞은 모두 0

"""## 모델 설계하기"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dense, SimpleRNN

model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=max_len-1)) # 10차원
model.add(SimpleRNN(32)) # hidden state
model.add(Dense(vocab_size, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(x, y, epochs=200, verbose=2)

# 문장을 생성하는 함수 만들어서 출력하기 # 입력한 단어로부터 다음 단어 예측하는 함수
def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수
    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장
    sentence = ''
    for _ in range(n): # n번 반복
        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩
        encoded = pad_sequences([encoded], maxlen=5, padding='pre') # 데이터에 대한 패딩
        result = model.predict_classes(encoded, verbose=0)
    # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.
        for word, index in t.word_index.items(): 
            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면
                break # 해당 단어가 예측 단어이므로 break
        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경
        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장
    # for문이므로 이 행동을 다시 반복
    sentence = init_word + sentence
    return sentence

#  입력한 단어로부터 다음 단어 예측
print(sentence_generation(model, t, '경마장에', 4))

print(sentence_generation(model, t, '그의', 2))

print(sentence_generation(model, t, '그의', 4))

print(sentence_generation(model, t, '가는', 5))

# 뒷 말로 앞 말을 예측하려하면 train이 안되어있기 때문에 임의로 예측할 것

"""# LSTM을 이용한 텍스트 생성"""

# /content/drive/MyDrive/NLP/ArticlesApril2018.csv

import pandas as pd
import numpy as np
from string import punctuation
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

path = '/content/drive/MyDrive/NLP/ArticlesApril2018.csv'
df = pd.read_csv(path)
df.head()

print('columns number: ', len(df.columns))
print(df.columns)

# null이 있는지 확인
df.info()

df['headline'].isnull().values.any()

# headline열에서 모든 신문기사의 제목을 뽑아 하나의 리스트로 저장
# title = list(df.columns)
# print(title)
headline = []
headline.extend(list(df.headline.values))

# 상위 5개만 출력
headline[:5]

# 총 샘플의 개수
print('총 샘플의 갯수 : {}'.format(len(headline)))

# unknown값을 가진 샘플 제거 후 개수 출력
# df['headline'].dropna(inplace=True)

headline = [n for n in headline if n != "Unknown"]
print('노이즈 값 제거 후 샘플의 갯수 : {}'.format(len(headline)))

# 기존에 출력했던 5개의 샘플 출력
headline[:5]

def repreprocessing(s):
  s = s.encode("utf8").decode("ascii", 'ignore')
  return ''.join(c for c in s if c not in punctuation).lower() # 구두점제거와 동시에 소문자화

text = [repreprocessing(x) for x in headline]
text[:5]

# 쓸데없는거 지웠으니 토큰화
t= Tokenizer()
t.fit_on_texts(text)
vocab_size = len(t.word_index) + 1
print('단어 집합의 크기 : %d' % vocab_size)

# 훈련데이터 구성
sequences = list()

for line in text:
  encoded = t.texts_to_sequences([line])[0] # 각 샘플에 대한 정수 인코딩
  for i in range(1, len(encoded)):
    sequence = encoded[:i+1]
    sequences.append(sequence)

sequences[:11]

'''
'former(99) nfl(269) cheerleaders(371) settlement offer 1 and a meeting with goodell',
 'epa to unveil a new rule its effect less science in policymaking',
 'the new noma explained',
 'how a bag of texas dirt  became a times tradition',
 'is school a place for selfexpression'
 '''
 # 데이터셋을 이런 식으로 만드는 이유 : 이전의 등장한 단어를 모두 참고하기 위해

# label 하기 전 과정...
index_to_word = {}

for key, value in t.word_index.items():
  index_to_word[value] = key

print('빈도수 상위 582번 단어: {}'.format(index_to_word[582]))

# padding

# 최대 길이 출력
max_len = max(len(l) for l in sequences)
print('샘플의 최대 길이 : {}'.format(max_len))

sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')
print(sequences[:3])

# label
sequences = np.array(sequences)
x = sequences[:, :-1]
y = sequences[:, -1]

print(x[:3])

print(y[:3])

y = to_categorical(y, num_classes=vocab_size)

# 여기까지 전처리
print(y)

"""## 모델 설계"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dense, LSTM

model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=max_len-1)) # vocab_size * 10, 한 단어당 10차원(보통 2의 배수로 지정)의 크기를 갖고 임베딩, -1: 맨 끝을 y로 분류(라벨링)했기 때문에
model.add(LSTM(128)) # 보통 2의 배수로 지정
model.add(Dense(vocab_size, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(x, y, epochs=200, verbose=2)
# loss=binary_crossentropy는 1dimension, input1....
# verbose=2 -> 0 = silent, 1 = progress bar, 2 = one line per epoch.

# 문장을 생성하는 함수 만들어서 출력하기 # 입력한 단어로부터 다음 단어 예측하는 함수
def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수
    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장
    sentence = ''
    for _ in range(n): # n번 반복
        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩
        encoded = pad_sequences([encoded], maxlen=23, padding='pre') # 데이터에 대한 패딩
        result = model.predict_classes(encoded, verbose=0)
    # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.
        for word, index in t.word_index.items(): 
            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면
                break # 해당 단어가 예측 단어이므로 break
        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경
        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장
    # for문이므로 이 행동을 다시 반복
    sentence = init_word + sentence
    return sentence

print(sentence_generation(model, t, 'i', 10))

print(sentence_generation(model, t, 'how', 10))

print(sentence_generation(model, t, 'former',10))

