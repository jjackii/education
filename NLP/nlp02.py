# -*- coding: utf-8 -*-
"""NLP02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VyckqxPIMqcUlXr8JrTeQghDaAROnlbg

## BPE (Byte Pair Encoding)
OOV에 대한 대처
"""

import re, collections

"""**defaultdict**\
https://www.geeksforgeeks.org/defaultdict-in-python/
"""

# collections, defaultdict
from collections import defaultdict
# Defining the dict 
d = defaultdict(int) 
     
L = [1, 1, 1, 2, 2, 2, 2, 2, 3, 4] 
print(d)    

# Iterate through the list 
# for keeping the count 
for i in L: 
         
    # The default value is 0 
    # so there is no need to  
    # enter the key first 
    d[i] += 1
         
print(d)

# Iteration count
num_merges = 10

# Definition of dict
dictionary = {'l o w </w>' : 5,
         'l o w e r </w>' : 2,
         'n e w e s t </w>':6,
         'w i d e s t </w>':3
         }

# dictionary = {'l o w <\w>': 5,
#               'l o w e r<\w>': 2,
#               'n e w e s t<\w>': 6,
#               'w i d e s t <\w>': 3
#               } # 임의로 count 지정
# error bad escape

"""> n_gram : n개의 단어 집합으로 만듦

e.g) 우리들은 밥을 먹었고, 나는 공부중이다.
- unigram
우리들은 / 밥을 / 먹었고, / 나는 / 공부중이다.
- bigram
우리들은 밥을 / 먹었고, 나는 / 공부중이다. space(\)
- trigram
우리들은 밥을 먹었고, / 나는 공부중이다. space(\)
"""

# 유니그램의 pair들의 빈도수를 카운트(without space)
def get_stats(dictionary):
  
  pairs = collections.defaultdict(int)

  for word, freq in dictionary.items():
    symbols = word.split()

    # Store(not update) characters with its frequency into pairs
    for i in range(len(symbols)-1):
      pairs[symbols[i], symbols[i+1]] += freq

  print('현재 pair들의 빈도수 :', dict(pairs))
  return pairs

get_stats(dictionary)

"""re.**escape**

`>>> print(re.escape('http://www.python.org'))`\
`http://www\.python\.org`

---
re.**compile**

```
*prog = **re.compile(**pattern)
result = *prog.match(string)
```
is same as
```
result = **re.match(**pattern, string)
```

---
re.**sub**

<str\> = re.sub(pattern, replace, string, count=0, flag=0)
"""

# Compare
# 가장 높은 freq의 piars쌍을 가져와 딕셔너리랑 비교 / best=pair, dictionary=v_in
def merge_dictionary(pair, v_in): 
    v_out = {}

    # 정규화해서 바이그램 만듦(두개씩 묶음)
    bigram = re.escape(' '.join(pair))

    # 앞뒤로 컴파일
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)') 

    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

## ??
bigram = re.escape(' '.join(best))
print(bigram)
p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)') 
for word in dictionary:
    w_out = p.sub(''.join(best), word)
    dictionary[w_out] = dictionary[word]
print(dictionary)

bpe_codes = {}
bpe_codes_reverse = {}

# 전체 몇번 반복 10
for i in range(num_merges):
  print(">> Step {0}".format(i+1))

  # 유니그램들의 페어 카운트 함수
  pairs = get_stats(dictionary)

  # 페어 중 가장 큰 카운트 값 best에 할당
  best = max(pairs, key=pairs.get)

  # 비교
  dictionary = merge_dictionary(best, dictionary)

  # 베스트카운트를  넣음
  bpe_codes[best] = i
  bpe_codes_reverse[best[0] + best[1]] = best

  print("new merge: {}".format(best))
  print("dictionary: {}".format(dictionary))

print(bpe_codes)
# bpe_code 출력하면 merge했던 기록이 출력

"""## Dealing with OOV (Out of Vocabulary)  
OOV 에 대처하기
"""

# OOV가 들어왔을 때 bpe가 대처하는 방법

# Return set of symbol pairs in a word.
# Word is represented as a tuple of symbols (symbols being variable-length strings)
def get_pairs(word):
    pairs = set()
    prev_char = word[0]
    for char in word[1:]:
        pairs.add((prev_char, char))
        prev_char = char
    return pairs

def encode(orig):
    """Encode word based on list of BPE merge operations, which are applied consecutively"""

    word = tuple(orig) + ('</w>',)

    # display(Markdown("__word split into characters:__ <tt>{}</tt>".format(word)))
    print("__word split into characters:__ <tt>{}</tt>".format(word))

    pairs = get_pairs(word)    

    if not pairs:
        return orig

    iteration = 0
    while True:
        iteration += 1
        # display(Markdown("__Iteration {}:__".format(iteration)))
        print("__Iteration {}:__".format(iteration))

        print("bigrams in the word: {}".format(pairs))
        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))
        print("candidate for merging: {}".format(bigram))
        if bigram not in bpe_codes:
            #display(Markdown("__Candidate not in BPE merges, algorithm stops.__"))
            print("__Candidate not in BPE merges, algorithm stops.__")
            break
        first, second = bigram
        new_word = []
        i = 0    
        while i < len(word):
            try:
              j = word.index(first, i)
              new_word.extend(word[i:j])
              i = j
            except:
              new_word.extend(word[i:])
              break

            if word[i] == first and i < len(word)-1 and word[i+1] == second:
                new_word.append(first+second)
                i += 2
            else:
                new_word.append(word[i])
                i += 1
        new_word = tuple(new_word)
        word = new_word
        print("word after merging: {}".format(word))
        if len(word) == 1:
            break
        else:
            pairs = get_pairs(word)

    # 특별 토큰인 </w>는 출력하지 않는다.
    if word[-1] == '</w>':
        word = word[:-1]
    elif word[-1].endswith('</w>'):
        word = word[:-1] + (word[-1].replace('</w>',''),)

    return word

encode('loki')

encode('lowest')

encode('lowing')

encode('highing')

# print('정확도 :', metrics.accuracy_score(y_test, y_pred))

"""# Tokenization of IMDB reviews 

Basically 
- raw -> encoded -> token
- token -> decoded -> raw

Tensorflow
"""

import tensorflow_datasets as tfds
import urllib.request
import pandas as pd

urllib.request.urlretrieve("https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv", filename="IMDb_Reviews.csv")

train_df = pd.read_csv('IMDb_Reviews.csv')

train_df['review']

train_df.head()

train_df.info()

# csv 리뷰 전체로 분절
tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_df['review'], target_vocab_size=2**13)

# 띄어쓰기는 _로, 분절은 띄어쓰기로 표현
print(tokenizer.subwords[:100])

# 21번째 샘플
print(train_df['review'][20])

# 21번째 샘플에 해당하는 것(한글) 정수로 바꿈(인코딩) : word2index처럼? : 유의미한 numbers
print('토큰화된 샘플 질문 : {}'.format(tokenizer.encode(train_df['review'][20])))

# 샘플 문장을 인코딩(맵핑)/디코딩할것
sample_string = "It's mind-blowing to me that this film was even made."

# 인코딩해서 저장
tokenized_string = tokenizer.encode(sample_string)
print('정수 인코딩 후의 문장 {}'.format(tokenized_string))

# 다시 디코딩 (원문 복구) '_'제거
original_string = tokenizer.decode(tokenized_string)
print('기존 문장 : {}'.format(original_string))

# 단어 집합의 크기
print('단어 집합의 크기(Vocab size) :', tokenizer.vocab_size) 

# 빈 방이 많다고 생각하면 될 듯. 2**13을 줬는데(차원) 문장은 짧으니까 ??

# 각각 맵핑된 결과
for ts in tokenized_string: # 임의로 가져온 샘플 문장
  print('{} ----> {}'.format(ts, tokenizer.decode([ts]))) # 에 대한 분절, 디코드(원본) 함께 출력

for ts in tokenized_string:
    print(f'{ts:<5} -----> {tokenizer.decode([ts])}')

# subword에는 xyz가 없으므로 각각 나눠놓음
sample_string = "It's mind-blowing to me that this film was evenxyz made."

tokenized_string = tokenizer.encode(sample_string)
print('정수 인코딩 후의 문장 {}'.format(tokenized_string))

original_string = tokenizer.decode(tokenized_string)
print('기존 문장 : {}'.format(original_string))

for ts in tokenized_string: 
  print(f'{ts:<5} -----> {tokenizer.decode([ts])}') # 새로 맵핑
# 7974 ----> even
# 8132 ----> x
# 8133 ----> y
# 997 ----> z

# tokenizer 객체를 다시 초기화하고 x, y, z 검색해보니까 원래 토큰화 자료에는 포함되어 있지 않네요
# 인코딩할때 tokenizer 객체에 없으면 추가하는것 같아요

# tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_df['review'], target_vocab_size= 2**13)
# 에서 생성되는 Vocabulary(딕셔너리), 안에는 x, y, z이 없네요. 들어있는 수도 8011개. 
# 어떤 알고리듬인지는 모르겠으나, encode/decode를 할때 Vocabulary를 사용하여,
# 문자열을 "숫자화"/"문자화" 시켜주는것 같읍니다. (저 숫자가 index로 인식하는건 오해일수있겠네요)

"""# TF-IDF (Python)
Term Frequency-Inverse Document Frequency\
: 각 단어들마다 중요한 정도를 가중치로 주는 방법\
https://wikidocs.net/31698

- used to convert sentences to vectors to `score` frequently appeared words. 

$$TF = \frac{term\ i\ frequency\ in\ sentence\ (=A)}{total\ words\ (=B)}$$

$$IDF = \ln[\frac{B}{A}]$$

$$score = TF * IDF$$

![link text](https://mungingdata.files.wordpress.com/2017/11/equation.png)
"""

import pandas as pd
from math import log

docs = [
        '먹고 싶은 사과',
        '먹고 싶은 바나나',
        '길고 노란 바나나 바나나',
        '저는 과일이 좋아요'
]

vocab = list(set(w for doc in docs for w in doc.split()))
vocab.sort()

print(vocab)

# num of sentences
N = len(docs)
print(N)

"""TF-IDF"""

# Function_1 : counts a word frequency
def tf(t,d):
  return d.count(t)

# Function_2 : counts inverse document frequency of a word
def idf(t):
  df = 0
  for doc in docs:
    df += t in doc
  return log(N/(df+1))

# Function_3 : calculates TF-IDF
# t=vocab, d=docs
def tfidf(t, d):
  return tf(t,d)*idf(t)

# run TF-IDF calculation for all the sentences in docs and store in df

result = []

for i in range(N):
  result.append([])

  d = docs[i]

  for j in range(len(vocab)):
    t = vocab[j]
    result[-1].append(tfidf(t,d))

tfidf_ = pd.DataFrame(result, columns = vocab)
tfidf_

# docs = [
#         '먹고 싶은 사과',
#         '먹고 싶은 바나나',
#         '길고 노란 바나나 바나나',
#         '저는 과일이 좋아요'
# ]

"""# TF-IDF (Scikit-learn) `TfidfVectorizer`"""

from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
        '먹고 싶은 사과',
        '먹고 싶은 바나나',
        '길고 노란 바나나 바나나',
        '저는 과일이 좋아요'
]

tfidfv = TfidfVectorizer().fit(corpus)

print(tfidfv.transform(corpus).toarray())
print(tfidfv.vocabulary_) # 위치비교 / 패키지별로 계산 방법이 달라서 값이 다른 것

"""# TF-IDF `TfidfVectorizer` on reuter's dataset
로이터 데이터로 TF-IDF 학습\
https://wikidocs.net/22933
"""

from tensorflow.keras.datasets import reuters
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# reuters dataset loading 10000 words with 80% Train set, 20% test set
(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)

print('훈련 샘플의 수 : {}'.format(len(x_train)))

print('테스트 샘플의 수 : {}'.format(len(x_test)))

# display first data set of train and test
print(x_train[0])
print(x_test[0])

# display first target class of train
print(y_train[0])

# display first target class of test
print(y_test[0])

# display number of target classes
num_classes = max(y_train) + 1
print('클래스의 수 : {}'.format(num_classes))

# few EDA: max, mean

print('훈련용 뉴스의 최대 길이 :{}'.format(max(len(l) for l in x_train)))
print('훈련용 뉴스의 평균 길이 : {}'.format(sum(map(len, x_train))/len(x_train)))

# few EDA : histogram
plt.hist([len(s) for s in x_train], bins= 50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

# few EDA: a variation of countplot

fig, axe = plt.subplots(ncols= 1)
fig.set_size_inches(12, 5)
sns.countplot(y_train)

# few EDA: unique elements, element counts
unique_elements, counts_elements = np.unique(y_train, return_counts=True)
print("각 클래스 빈도수 : ")
print(np.asarray((unique_elements, counts_elements)))

# download reuter word index
word_index = reuters.get_word_index(path="reuters_word_index.json")

# word index of 'the'
word_index['the']

# word index of 'it'
word_index['it']

# index to word simulation
index_to_word = {index +3 : word for word, index in word_index.items()}

# retriving words with its indexes
print(index_to_word[4])
print(index_to_word[16])

# inserting "Secret" index tags

# 0 <pad>
# 1 <sos>
# 2 <unk>
for index, token in enumerate(("<pad>", "<sos>", "<unk>")):
  index_to_word[index] = token

# Decoding a training news
print(' '.join([index_to_word[index] for index in x_train[0]]))

# Decoding training data
decoded = []
for i in range(len(x_train)):
  t = ' '.join([index_to_word[index] for index in x_train[i]])
  decoded.append(t)

x_train = decoded

# Decoding test data
for i in range(len(x_test)):
  t = ' '.join([index_to_word[index] for index in x_test[i]])
  decoded.append(t)

x_test = decoded

# Display 5 training data
x_train[:5]

# Display 5 test data
x_test[:5]

"""sk TF-IDF"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(x_train)
# fit_transform : 학습할 때와 동일한 기반 설정으로 동일하게 테스트 데이터를 변환해야 하는 것

#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

#- 나이브 베이즈 분류기를 수행합니다.
#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.
clf = MultinomialNB().fit(X_train_tfidf, y_train)

def tfidf_vectorizer(data):
  data_counts = count_vect.transform(data)
  data_tfidf = tfidf_transformer.transform(data_counts)
  return data_tfidf

y_pred = clf.predict(tfidf_vectorizer(x_test))
print(metrics.classification_report(y_test, y_pred))

