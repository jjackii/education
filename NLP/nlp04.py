# -*- coding: utf-8 -*-
"""NLP04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W-bkwyhWdYY0e3ibHZWRXhvVNaWY0w6t

DAY04 - LSA와 LDA

# 문장 4개로 LSA 실습하기
"""

import pandas as pd
import numpy as np
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import Normalizer

# freq of DTM <- countvectorizer
# DTM을 얻는 함수
def get_dtm(sentences):
  # countvectorizer객체 생성. kor지원(X)....
  vectorizer = CountVectorizer(min_df=1, stop_words='english', dtype=np.float32)
  dtm = vectorizer.fit_transform(sentences)
  return dtm, vectorizer

sentences = ['중앙방역대책본부는 오늘 0시 기준 코로나19 확진 환자가 44명 추가 확인돼 모두 13,417명으로 늘었다고 밝혔습니다.', 
                 '신규 확진 환자 가운데 해외 유입 사례는 23명, 국내 발생은 21명입니다.']
vectorizer = CountVectorizer(min_df=1, stop_words='english', dtype=np.float32)
dtm = vectorizer.fit_transform(sentences)
vectorizer.vocabulary_
# vectorizer.get_feature_names()

# func : print DTM matrix
def print_dtm_matrix(dtm, vectorizer, sentences):
  return pd.DataFrame(dtm.toarray(), index=sentences, columns = vectorizer.get_feature_names())

def lsa_tsvd(n_components, dtm, vectorizer, sentences): # n_components : 문서의 갯수
  lsa = TruncatedSVD(n_components, algorithm='arpack')# TruncatedSVD : 상위 몇%까지?

  # lsa에서 만든거 할당  
  dtm_lsa = lsa.fit_transform(dtm)

  # dt_lsa를 표준화? 후 할당  
  dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)

  components = pd.DataFrame(lsa.components_, index=['components-1', 'components-2'], columns=vectorizer.get_feature_names())
  lsa = pd.DataFrame(dtm_lsa, index=sentences, columns=['components-1','components-2'])

  # similarity
  similarity = np.asarray(np.asmatrix(dtm_lsa) * np.asmatrix(dtm_lsa).T)
  similarity = pd.DataFrame(similarity, index=sentences, columns=sentences)

  return components, lsa, similarity

# component : 구성요소

# main 불러오기 -> sen으로 dtm과 ?를 뽑아내는 작업
def main():
      sentences = ['중앙방역대책본부는 오늘 0시 기준 코로나19 확진 환자가 44명 추가 확인돼 모두 13,417명으로 늘었다고 밝혔습니다.', 
                 '신규 확진 환자 가운데 해외 유입 사례는 23명, 국내 발생은 21명입니다.', 
                 '국내 발생 환자는 지역별로 서울 7명, 경기 8명 등 수도권에서 15명이 확인됐고, 광주에서도 5명, 대전에서도 1명이 확진됐습니다.', 
                 '또한 코로나19로 1명이 추가 사망해 누적 사망자는 모두 289명으로 늘었습니다.'] # 코로나 19로
      dtm, vectorizer = get_dtm(sentences)
      d = print_dtm_matrix(dtm, vectorizer, sentences)
      c, l, s = lsa_tsvd(n_components=2, dtm=dtm, vectorizer=vectorizer, sentences=sentences)
      return d, c, l, s

d, c, l, s = main()

d

c

"""# Sklearn dataset 'fetch_20newsgroups' 활용한 LSA 실습"""

import pandas as pd
from sklearn.datasets import fetch_20newsgroups

dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers','footers', 'quotes'))
documents = dataset.data

# error dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('header', 'footer', 'quotes'))

new_df = pd.DataFrame({'document': documents})
# d = {'col1':[1,2], 'col2':[3,4]}

new_df['clean_doc'] = new_df['document'].str.replace("[^a-zA-Z]"," ")
new_df['clean_doc'] = new_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3])) # 길이 3 이상만 split
new_df['clean_doc'] = new_df['clean_doc'].apply(lambda x: x.lower())
new_df

import nltk
nltk.download('all')

from nltk.corpus import stopwords

stop_words = stopwords.words('english')
tokenized_doc = new_df['clean_doc'].apply(lambda x: x.split())
tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) # stopwords가 아닌것은 남겨놓고 맞는것은 제외시킨다

# 토큰화된거 합치는 과정
detokenized_doc = []
for i in range(len(new_df)):
  t = ' '.join(tokenized_doc[i])
  detokenized_doc.append(t)

new_df['clean_doc'] = detokenized_doc

from sklearn.feature_extraction.text import TfidfVectorizer

Vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, max_df=0.5, smooth_idf=True) # (분모) +1

X = Vectorizer.fit_transform(new_df['clean_doc'])

from sklearn.decomposition import TruncatedSVD

svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)
svd_model.fit(X)

terms = Vectorizer.get_feature_names()

def get_topics(components, feature_names, n=5):
  for idx, topic in enumerate(components):
        print("Topic %d:" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])

get_topics(svd_model.components_, terms)
# 가장 중요한 단어 5개 + sort

"""# LDA
https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#4whatdoesldado

LSA : DTM을 차원 축소 하여 축소 차원에서 근접 단어들을 토픽으로 묶는다.\
LDA : 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출한다\
https://wikidocs.net/30708
"""

import pandas as pd
from sklearn.datasets import fetch_20newsgroups

dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers','footers','quotes')) # 인용구제거
documents = dataset.data

news_df = pd.DataFrame({'document': documents})
news_df['clean_doc'] = news_df['document'].str.replace("[^a-zA-Z]"," ")
news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))
news_df['clean_doc'] = news_df['document'].apply(lambda x: x.lower())

# doc를 토큰화하는 작업
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())
tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])

from gensim import corpora
dictionary = corpora.Dictionary(tokenized_doc) #사전완성
corpus = [dictionary.doc2bow(text) for text in tokenized_doc] # doc2bow

corpus # word -> number / store to corpus as (number, freq)

import gensim
num_topics = 20
k = 20
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)
topics = ldamodel.print_topics(num_words=4) # 상위 4개만

#https://colab.research.google.com/drive/1izhb6D7IJfkhcN7vcV69j-wPYvNX2Ggx

for topic in topics:
  print(topic)

print('Perplexity: ', ldamodel.log_perplexity(corpus)) # 내부 평가 지표, 숫자가 낮을수록 좋은 성능

# 코히런스모델? 코히런스가 높으면 주제 파악이 쉽다.
from gensim.models.coherencemodel import CoherenceModel

coherence_model_lda = CoherenceModel(model=ldamodel, texts=tokenized_doc, dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score : ', coherence_lda) # 0.5803686335572611 좋지도 나쁘지도 x

"""https://coredottoday.github.io/2018/09/17/%EB%AA%A8%EB%8D%B8-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D/"""

print(ldamodel.show_topics(formatted=False))

pprint(ldamodel.show_topics(formatted=False))

import pprint
pp = pprint.PrettyPrinter(indent=2)
pp.pprint(ldamodel.show_topics(formatted=False))



"""# 실제 뉴스 데이터로 뉴스 기사 주제를 분류하는 텍스트 분류기 만들기

## download
"""

!pip install beautifulsoup4
!pip install newspaper3k
!pip install konlpy

! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git

ls

!pwd

cd Mecab-ko-for-Google-Colab

ls

! bash install_mecab-ko_on_colab190912.sh

from konlpy.tag import Mecab
tokenizer = Mecab()

"""## BeautifulSoup 패키지"""

from bs4 import BeautifulSoup

html = '''
<html>
  <head>
  </head>
  <body>
    <h1> 장바구니
      <p id='clothes' class='name' title='라운드티'> 라운드티
        <span class = 'number'> 25 </span>
        <span class = 'price'> 29000 </span>
        <span class = 'menu'> 의류 </span>
        <a href = 'http://www.naver.com'> 바로가기 </a>
      </p>
      <p id='watch' class='name' title='시계'> 시계
        <span class='number'> 28 </span>
        <span class='price'> 32000 </span>
        <span class='menu'> 악세서리 </span>
        <a herf = 'http://www.facebook.com'>바로가기 </a>
      </p>
    </h1>
  </body>
</html>
'''
soup = BeautifulSoup(html, 'html.parser')

print(soup.select('body'))

print(soup.select('p'))

print(soup.select('h1 .name .menu'))

print(soup.select('html > h1'))

a = soup.select('html > h1')
for i in a:
  print(i)

print(soup.select('html > body > h1'))

"""## newspaper3k 패키지"""

from newspaper import Article

url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=030&aid=0002881076'

article = Article(url, language='ko')
article.download()
article.parse()

print('title of article :')
print(article.title)
print('')

print('contents of article')
print(article.text)

"""## BeautifulSoup와 newspaper3k를 통해 크롤러 만들기"""

import requests
import pandas as pd
from bs4 import BeautifulSoup

def make_urllist(page_num, code, date):

  urllist = []

  for i in range(1, page_num+1):
    url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)
    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36'}
    news = requests.get(url, headers=headers)

    soup = BeautifulSoup(news.content, 'html.parser')

    # 1
    news_list = soup.select('.newsflash_body .type06_headline li dl')

    # 2
    news_list.extend(soup.select('.newsflash_body .type06 li dl'))

    for line in news_list:
      urllist.append(line.a.get('href'))
      
  return urllist

url_list = make_urllist(2, 101, 20200506) # 20년5월6일 101(경제)기사 2페이지
print('뉴스 기사의 개수 :', len(url_list))

url_list[:5]

idx2word= {'101' : '경제', '102' : '사회', '103': '생활/문화', '105': 'IT/과학'}

from newspaper import Article

# newspaper3k를 통해서 만들어진 함수로 url리스트와 해당 url이
# 어떤 카테고리인지 코드를 알려주면 이를 통해서 데이터프레임을 생성하는 함수
def make_data(urllist, code): # urllist = url_list
  text_list = []
  for url in urllist:
    article = Article(url, language='ko')
    article.download()
    article.parse()
    text_list.append(article.text)

  # df의 'news'키 아래 parsing한 것을 value로 붙여줌
  df = pd.DataFrame({'news': text_list})

  # df의 'code' 키 아래 한글 카테고리명을 붙여줌
  df['code'] = idx2word[str(code)]
  return df

text_list2 = []

url2 = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=057&aid=0001451723'
article2 = Article(url2, language='ko')
article2.download()
article2.parse()

text_list2.append(article2.text)

  # parsing한 것을 value로 붙여줌
df2 = pd.DataFrame({'news':text_list2})

df2['code'] = idx2word[str(101)]

df2

data = make_data(url_list, 101)
data[:10]

"""## Data 수집 및 전처리"""

code_list = [102, 103, 105]
code_list

# date, page_num
def make_total_data(page_num, code_list, date):
  df = None
  
  for code in code_list:
    url_list = make_urllist(page_num, code, date)
    df_temp = make_data(url_list, code)
    
    print(str(code)+'번 코드에 대한 데이터를 만들었습니다.')

    if df is not None:
      df = pd.concat([df, df_temp])
    else:
      df = df_temp

  return df

df = make_total_data(1, code_list, 20200506) # 한페이지만

print('뉴스 기사의 개수 :', len(df)) # 한페이지당 20개 * 3

df.sample(10)

"""## 대량 크롤링.."""

df = make_total_data(100, code_list, 20200506) # 1시간!!
# 기사 일자를 바꿔보면서 데이터를 모으면 더욱 다양한 데이터를 얻을 수 있음
# 위에서 크롤링한거에 100배 분량 크롤링

# # 저장한 csv파일 불러오기
# csv_path = '경로'
# df.to_csv(csv_path, index = False)

# if os.path.exists(csv_path):
#   print(f'{csv_path}File saved')

df.head()

# 정규 표현식을 이용해 한글 외 문자 제거
df['news'] = df['news'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣0-9]', ' ')
df['news']

# 1)import re
# def only_korean(text):
#     korean = re.compile('[^ ㄱ-ㅣ 가-힣 0-9]+')
#     result = korean.sub(' ', text)
#     return result
# df['clean_news'] = df['news'].apply(only_korean)

# !! 2)df['news'] = [re.sub('[^A-Za-z0-9가-힣]', '', w) for w in df['news']]

# 3)df['news'] = df['news'].str.replace("[^ \u3131-\u3163\uac00-\ud7a3]"," ") # unicode

# 데이터에 null 값이 있는지 없는지 확인
df[df['news'].isnull()]
# print(df.isnull().sum())

# 중복된 샘플 제거
print(len(df), '\n')
df.nunique()

df.drop_duplicates(subset=['news'],  inplace=True)
print('뉴스 기사의 개수 :', len(df))

"""## 데이터 탐색"""

df['code'].value_counts().plot(kind='bar')

print(df.groupby('code').size().reset_index(name='count'))

"""## 토큰화(Mecab)"""

from konlpy.tag import Mecab
tokenizer = Mecab()

kor_text = '밤에 귀가하던 여성에게 범죄를 시도한 대 남성이 구속됐다서울 제주경찰서는 \
            상해 혐의로 씨를 구속해 수사하고 있다고 일 밝혔다씨는 지난달 일 피해 여성을 \
            인근 지하철 역에서부터 따라가 폭행을 시도하려다가 도망간 혐의를 받는다피해 \
            여성이 저항하자 놀란 씨는 도망갔으며 신고를 받고 주변을 수색하던 경찰에 \
            체포됐다피해 여성은 이 과정에서 경미한 부상을 입은 것으로 전해졌다'

print(tokenizer.morphs(kor_text))

"""## 불용어(stopwords)제거"""

stopwords = ['에','는','은','을','했','에게','있','이','의','하','한','다','과','때문','할','수','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']

#토큰화 및 토큰화 과정에서 불용어를 제거하는 함수
def preprocessing(data):
  text_data = []

  for sentence in data:
    temp_data =[]
    # 토큰화
    temp_data = tokenizer.morphs(sentence)
    #불용어 제거
    temp_data = [word for word in temp_data if not word in stopwords]
    text_data.append(temp_data)
  text_data = list(map(' '.join, text_data))
  return text_data

text_data = preprocessing(df['news'])

print(text_data[0])

"""## 텍스트 유사도"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

x_train, x_test, y_train, y_test = train_test_split(text_data, df['code'], random_state=0)

print('훈련용 뉴스 기사의 갯수 :', len(x_train))
print('테스트용 뉴스 기사의 갯수 :', len(x_test))
print('훈련용 레이블의 갯수 :', len(y_train))
print('테스트용 레이블의 갯수 :', len(y_test))

"""## 인코딩(텍스트간의 유사도)

숙제
- word2vec
- glove
- fastext
"""

# count (후에!)-> tfidf 사용 가능

# dtm
count_vect = CountVectorizer()
x_train_counts = count_vect.fit_transform(x_train)
# fit_transform : 학습할 때와 동일한 기반 설정으로 동일하게 테스트 데이터를 변환해야 하는 것

# tfidf
tfidf_transformer = TfidfTransformer()
x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)

"""## 모델 (머신러닝 - 나이브 베이즈 분류기)"""

# x train TF-IDF벡터, y train 레이블
# 나이브 베이즈 분류기를 수행
clf = MultinomialNB().fit(x_train_tfidf, y_train) # 객체생성()후 fit

"""숙제 (MultinomialNB 대신)
1. 로지스틱 회귀\
LogisticRegression
2. 결정트리\
DecisionTreeClassifier
3. 랜덤 포레스트\
RandomForestClassifier
4. 그래디언트 부스팅 트리\
GradientBoostingClassifier
"""

def tfidf_vectorizer(data):
  data_counts = count_vect.transform(data)
  data_tfidf = tfidf_transformer.transform(data_counts)
  return data_tfidf

"""## 예측"""

new_sent = preprocessing(["민주당 일각에서 법사위의 체계·자구 심사 기능을 없애야 한다는 \
                           주장이 나오는 데 대해 “체계·자구 심사가 법안 지연의 수단으로 \
                          쓰이는 것은 바람직하지 않다”면서도 “국회를 통과하는 법안 중 위헌\
                          법률이 1년에 10건 넘게 나온다. 그런데 체계·자구 심사까지 없애면 매우 위험하다”고 반박했다."])

print(clf.predict(tfidf_vectorizer(new_sent)))

new_sent = preprocessing(["인도 로맨틱 코미디 영화 <까립까립 싱글>(2017)을 봤을 때 나는 두 눈을 의심했다. \
                          저 사람이 남자 주인공이라고? 노안에 가까운 이목구비와 기름때로 뭉친 파마머리와, \
                          대충 툭툭 던지는 말투 등 전혀 로맨틱하지 않은 외모였다. 반감이 일면서 \
                          ‘난 외모지상주의자가 아니다’라고 자부했던 나에 대해 회의가 들었다.\
                           티브이를 꺼버릴까? 다른 걸 볼까? 그런데, 이상하다. 왜 이렇게 매력 있지? 개구리와\
                            같이 툭 불거진 눈망울 안에는 어떤 인도 배우에게서도 느끼지 못한 \
                            부드러움과 선량함, 무엇보다 슬픔이 있었다. 2시간 뒤 영화가 끝나고 나는 완전히 이 배우에게 빠졌다"])

print(clf.predict(tfidf_vectorizer(new_sent)))

new_sent = preprocessing(["20분기 연속으로 적자에 시달리는 LG전자가 브랜드 이름부터 성능, 디자인까지 대대적인 변화를 \
                          적용한 LG 벨벳은 등장 전부터 온라인 커뮤니티를 뜨겁게 달궜다. 사용자들은 “디자인이 예쁘다”, \
                          “슬림하다”는 반응을 보이며 LG 벨벳에 대한 기대감을 드러냈다."])

print(clf.predict(tfidf_vectorizer(new_sent)))

"""## 평가"""

y_pred = clf.predict(tfidf_vectorizer(x_test))

print(metrics.classification_report(y_test, y_pred))

"""http://seb.kr/w/F1_%EC%8A%A4%EC%BD%94%EC%96%B4"""

