# -*- coding: utf-8 -*-
"""NLP01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rolN_l304S99QAVTpFQtCqqYdA1mqDdc

## 노이즈 유형 1
"""

def pad_punctuation(sentence, punc):
  for p in punc:
    sentence = sentence.replace(p, ' '+p+' ')

  return sentence

sentence = 'Hi, my name is john.'

print(pad_punctuation(sentence, ['.', '?', '!', ',']))

"""## 노이즈 유형 2"""

sentence = 'First, open the first chapter.'
print(sentence.lower())

sentence = 'First, open the first chapter.'
print(sentence.upper())

"""## 노이즈 유형 3"""

import re

sentence = 'He is a ten-year-old boy.'
sentence = re.sub('([^a-zA-Z.,?!])', ' ', sentence)

print(sentence)

# https://www.gutenberg.org/files/2397/2397-h/2397-h.htm

corpus = \
"""
  To ALEXANDER GRAHAM BELL

  Who has taught the deaf to speak
  and enabled the listening ear to hear speech
  from the Atlantic to the Rockies,
  I dedicate this Story of My Life.
"""

def cleaning_text(text,  punc, regex):
  # 노이즈 유형 1 문장 부호 공백추가
  for p in punc:
    text = text.replace(p, ' '+p+' ')

  # 노이즈 유형 2, 3 소문자화 및 특수문자 제거
  text = re.sub(regex, ' ', text).lower()

  return text

print(cleaning_text(corpus, ['.', ',', '!', '?'], '([^a-zA-Z0-9.,?\n])'))

! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git

ls

cd Mecab-ko-for-Google-Colab/

ls

! bash install_mecab-ko_on_colab190912.sh

!pip install konlpy

from konlpy.tag import Mecab

mecab = Mecab()

print(mecab.morphs('파일을로컬에받아놓아도다운이되는건가?'))

# Commented out IPython magic to ensure Python compatibility.
import os
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np

# %matplotlib inline

# from google.colab import files
# uploaded = files.upload()
# print(uploaded.keys())

# for fn in uploaded.keys():
#   print('User uploaded file "{name}" with length {length} bytes'.format(
#       name=fn, length=len(uploaded[fn])))
# # 파일 수동으로 넣기

from google.colab import drive
drive.mount('/gdrive', force_remount=True)

!ls -al /gdrive/'My Drive'

path_to_file = '/gdrive/My Drive/korean-english-park.train.ko'

with open(path_to_file, 'r', encoding='utf-8') as f:
  raw = f.read().splitlines()

  print('Data Size', len(raw))

print('Example:')
for sen in raw[0:100] [::20]: print('>>', sen)

# 시퀀스객체[시작인덱스::증가폭]
# 시퀀스객체[시작인덱스:끝인덱스] 리스트의 일부를 잘라서 새 리스트를 만듦

raw[20]

len(raw[1]) # 94123, 36, 52, 26
# type(raw) # list

# 초기화
min_len = 999
max_len = 0
sum_len = 0

for sen in raw:
  length = len(sen)
  if min_len > length: min_len = length
  if max_len < length: max_len = length
  sum_len += length

print('문장의 최단 길이:', min_len)
print('문장의 최장 길이:', max_len)
print('문장의 평균 길이:', sum_len//len(raw))

sentence_length = np.zeros((max_len), dtype=np.int)

for sen in raw:
  sentence_length[len(sen)-1] += 1

plt.bar(range(max_len), sentence_length, width=1.0)
plt.title('Sentence Length Distribution')

def check_sentence_with_length(raw, length):
  count = 0

  for sen in raw:
    if len(sen) == length:
      print(sen)
      count += 1
      if count > 100: return

check_sentence_with_length(raw, 1)

for idx, _sum in enumerate(sentence_length):
  # 한 문장의 길이가 1500을 초과하는 문장의 인덱스를 추출
  if _sum > 1500:
    print('Outlier index:', idx+1)

check_sentence_with_length(raw, 11)

min_len = 999
max_len = 0
sum_len = 0

cleaned_corpus = list(set(raw)) # set : 중복을 허용하지 않는 타입 -> set 하고 list

print("Data Size:", len(cleaned_corpus))

for sen in cleaned_corpus:
  length = len(sen)
  if min_len > length: min_len = length
  if max_len < length: max_len = length
  sum_len += length

print('문장의 최단 길이:', min_len)
print('문장의 최장 길이:', max_len)
print('문장의 평균 길이:', sum_len//len(cleaned_corpus))

sentence_length = np.zeros((max_len), dtype=np.int)

check_sentence_with_length(cleaned_corpus, 11)

max_len = 150
min_len = 10

filtered_corpus = [s for s in cleaned_corpus if (len(s)<max_len) & (len(s)>=min_len)]

sentence_length = np.zeros((max_len), dtype=np.int)

for sen in filtered_corpus:
  sentence_length[len(sen)-1] += 1

plt.bar(range(max_len), sentence_length, width=1.0)
plt.title('Sentence Length Distribution')
plt.show()



"""## 공백 기반 토큰화"""

def tokenize(corpus):
  tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')
  tokenizer.fit_on_texts(corpus)

  tensor = tokenizer.texts_to_sequences(corpus)
  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')
  return tensor, tokenizer

# 정제된 데이터를 공백 기반으로 토큰화하여 저장하는 코드
split_corpus = []

for kor in filtered_corpus:
  split_corpus.append(kor.split())

split_tensor, split_tokenizer = tokenize(split_corpus)
print('Split Vocab Size:', len(split_tokenizer.index_word))

# 나는 밥을 먹었다
# 나는00
# 밥을00
# 먹었다
# [나는, 밥을, 먹었다]

split_tensor, split_tokenizer = tokenize(split_corpus)
print('Split Vocab Size:', len(split_tokenizer.index_word))

split_tensor, split_tokenizer = tokenize(filtered_corpus)
print('Split Vocab Size:', len(split_tokenizer.index_word))

for idx, word in enumerate(split_tokenizer.word_index):
  print(idx, ':', word)

  if idx > 10: break

## 유사한 의미를 지니고 있음에도 다른것으로 구분힘
## 밝 + 혔다 // 밝 + 히다, 밝 + 다

# 위에서 사용한 코드를 활용해 MeCab 단어 사전 만들기
# Hint : mecab.morphs()를 사용해 형태소 분석
def mecab_split(sentence):
  return mecab.morphs(sentence)

mecab_corpus = []

for kor in filtered_corpus:
  mecab_corpus.append(mecab_split(kor))

mecab_tensor, mecab_tokenizer = tokenize(mecab_corpus)
print("MeCab Vocab Size", len(mecab_tokenizer.index_word))

# 한국어 처리를 할 때 공백 기반 토큰화는 절대절대절대적으로 지양!! vs 형태소 토큰화 (library ex,mecab)
# 토큰화하다 : 분절
# mecab 의 형태소 기반 토큰화인데 이게 사이즈는 더 커도 이걸로 하는게 낫다
# 공백기반으로는 237435개 토큰화된거고 Mecab으로 형태소 기반은  52279사이즈로 토큰화 된거



"""
## 1) tokenizer.squences_to_texts() 함수를 사용하여 Decoding

## 2) tokenizer.index_word를 사용하여 Decoding

두 가지 방법으로 mecab_tensor[100]을 원문으로 되돌리기 (띄어쓰기 고려X)"""

# Case1 : tokenizer.squences_to_texts()

# Case1
texts = mecab_tokenizer.sequences_to_texts([mecab_tensor[100]]) # mecab 안에서 random하게 생성된 문장 중 100번째 문장
print(texts[0])

# Case2 :tokenizer.index_word

sentence = ' '

for w in mecab_tensor[100]:
  if w == 0: continue
  sentence += mecab_tokenizer.index_word[w] + ' '

print(sentence)

